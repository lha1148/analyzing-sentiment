{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63IsEuTCh2E_",
        "outputId": "f26a7092-8eb3-4777-cd00-eebe09c4b88c"
      },
      "id": "63IsEuTCh2E_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "\n",
        "teencode_dict = {\n",
        "    \"hok\": \"kh√¥ng\",\n",
        "    \"k\": \"kh√¥ng\",\n",
        "    \"ko\": \"kh√¥ng\",\n",
        "    \"dc\": \"ƒë∆∞·ª£c\",\n",
        "    \"vl\": \"v√£i\",\n",
        "    \"thik\": \"th√≠ch\",\n",
        "    \"bt\": \"b√¨nh th∆∞·ªùng\",\n",
        "    \"j\": \"g√¨\",\n",
        "    \"mik\": \"m√¨nh\",\n",
        "    \"bn\": \"b·∫°n\",\n",
        "    \"vs\": \"v·ªõi\",\n",
        "    \"<3\": \"y√™u\"\n",
        "}\n",
        "\n",
        "def replace_teencode(text):\n",
        "    text = text.lower()\n",
        "    for teencode, replacement in teencode_dict.items():\n",
        "        text = re.sub(r'\\b' + re.escape(teencode) + r'\\b', replacement, text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = emoji.replace_emoji(text, replace='')  # lo·∫°i b·ªè emoji\n",
        "    text = replace_teencode(text)\n",
        "    text = re.sub(r\"‚Ä¶\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s,.?!\\-]\", \"\", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "text = \"Ko bt mik thik c√°i n√†y vl <3 üòÇ :))) www.google.com\"\n",
        "print(clean_text(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2syJEQK0hwI9",
        "outputId": "bbe50898-657b-4518-a80c-3a4e2a7e7ec0"
      },
      "id": "2syJEQK0hwI9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kh√¥ng b√¨nh th∆∞·ªùng m√¨nh th√≠ch c√°i n√†y v√£i 3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-08T07:16:58.516753Z",
          "start_time": "2024-12-08T07:16:56.880517Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2862ab2335dff1",
        "outputId": "705e01db-0d6f-4659-8ea7-04310bc15eb7"
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk random\n"
      ],
      "id": "2f2862ab2335dff1",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-08T07:37:52.464340Z",
          "start_time": "2024-12-08T07:37:52.461245Z"
        },
        "id": "23756066b2d28e24"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import nltk"
      ],
      "id": "23756066b2d28e24",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vcZE_uwbfLd",
        "outputId": "604c8ffc-2c6a-40f2-c6ec-f445a53fe9dd"
      },
      "id": "0vcZE_uwbfLd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pT8gJkQ7dDHN"
      },
      "id": "pT8gJkQ7dDHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# ƒê·ªçc file txt\n",
        "with open(\"/content/drive/MyDrive/NLP/data/word_net_vi.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# X·ª≠ l√Ω d·ªØ li·ªáu: t√°ch c√°c t·ª´ ƒë·ªìng nghƒ©a tr√™n m·ªói d√≤ng\n",
        "synonyms_list = []\n",
        "for line in lines:\n",
        "    # B·ªè kho·∫£ng tr·∫Øng d∆∞ v√† t√°ch theo d·∫•u ph·∫©y\n",
        "    words = [word.strip() for word in line.strip().split(\",\") if word.strip()]\n",
        "    if words:\n",
        "        synonyms_list.append(words)\n",
        "\n",
        "# Ghi ra file JSON\n",
        "with open(\"/content/drive/MyDrive/NLP/data/word_net_vi.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(synonyms_list, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng sang word_net_vi.json\")\n"
      ],
      "metadata": {
        "id": "lczxBdgudF6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83737605-695c-442a-8baf-6fa0893b23fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng sang word_net_vi.json\n"
          ]
        }
      ],
      "id": "lczxBdgudF6N"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-08T07:37:53.978140Z",
          "start_time": "2024-12-08T07:37:53.937054Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f11ec7c944a50be",
        "outputId": "a7d24584-27cf-43b3-c086-d716927fe195"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# N·∫°p d·ªØ li·ªáu v√†o\n",
        "data = pd.read_csv(r\"/content/drive/MyDrive/NLP/data/processed_sentiment_data_no_emojis.csv\")\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c ph·∫ßn ph·ª• thu·ªôc c·ªßa NLTK\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ƒê·ªçc file word_net__vi.json\n",
        "with open(r'/content/drive/MyDrive/NLP/data/word_net_vi.json', 'r', encoding='utf-8') as f:\n",
        "    word_net = json.load(f)\n"
      ],
      "id": "1f11ec7c944a50be",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def check_label_distribution(data):\n",
        "    # ƒê·∫øm s·ªë l∆∞·ª£ng c√°c nh√£n (sentiment)\n",
        "    label_counts = data['sentiment'].value_counts()\n",
        "    print(\"Ph√¢n b·ªë nh√£n trong d·ªØ li·ªáu:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    return label_counts\n",
        "\n",
        "label_counts = check_label_distribution(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phh4s_VQfRzH",
        "outputId": "14d9344c-df43-4d46-abf5-24ffba3300b4"
      },
      "id": "Phh4s_VQfRzH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ph√¢n b·ªë nh√£n trong d·ªØ li·ªáu:\n",
            "sentiment\n",
            "T·ªët           1610\n",
            "T·ªá            1269\n",
            "Trung t√≠nh     184\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-12-08T07:37:55.776844Z",
          "start_time": "2024-12-08T07:37:55.771519Z"
        },
        "id": "initial_id"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# H√†m th√™m t·ª´ ƒë·ªìng nghƒ©a v√†o c√¢u (RI)\n",
        "def random_insertion(text):\n",
        "    words = text.split()\n",
        "\n",
        "    # Ch·ªçn ng·∫´u nhi√™n m·ªôt v·ªã tr√≠ trong c√¢u\n",
        "    index = random.randint(0, len(words) - 1)\n",
        "\n",
        "    # L·∫•y t·ª´ ·ªü v·ªã tr√≠ n√†y\n",
        "    word = words[index]\n",
        "\n",
        "    # Ki·ªÉm tra xem t·ª´ n√†y c√≥ trong word_net kh√¥ng\n",
        "    if word in word_net and word_net[word]:  # Ki·ªÉm tra xem t·ª´ c√≥ ƒë·ªìng nghƒ©a kh√¥ng\n",
        "        synonym = random.choice(word_net[word])\n",
        "        words.insert(index, synonym)  # Th√™m t·ª´ ƒë·ªìng nghƒ©a v√†o v·ªã tr√≠ ng·∫´u nhi√™n\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# H√†m ho√°n ƒë·ªïi ng·∫´u nhi√™n hai t·ª´ trong c√¢u (RS)\n",
        "def random_swap(text):\n",
        "    words = text.split()\n",
        "\n",
        "    # Ki·ªÉm tra n·∫øu c√≥ √≠t nh·∫•t 2 t·ª´ trong c√¢u\n",
        "    if len(words) < 2:\n",
        "        return text  # Tr·∫£ v·ªÅ c√¢u g·ªëc n·∫øu c√≥ √≠t h∆°n 2 t·ª´\n",
        "\n",
        "    # Ch·ªçn ng·∫´u nhi√™n hai t·ª´ trong c√¢u\n",
        "    index1, index2 = random.sample(range(len(words)), 2)\n",
        "\n",
        "    # Ho√°n ƒë·ªïi v·ªã tr√≠ c·ªßa hai t·ª´\n",
        "    words[index1], words[index2] = words[index2], words[index1]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# H√†m tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng c·∫£ hai k·ªπ thu·∫≠t RI v√† RS\n",
        "def augment_data(text, augmentation_type='both'):\n",
        "    if augmentation_type == 'RI':\n",
        "        return random_insertion(text)\n",
        "    elif augmentation_type == 'RS':\n",
        "        return random_swap(text)\n",
        "    elif augmentation_type == 'both':\n",
        "        return random_swap(random_insertion(text))\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# C√¢n b·∫±ng d·ªØ li·ªáu cho nh√£n \"Trung t√≠nh\"\n",
        "def balance_neutral_data(data, target_count):\n",
        "    # L·ªçc c√°c c√¢u c√≥ nh√£n 'Trung t√≠nh'\n",
        "    neutral_data = data[data['sentiment'] == 'Trung t√≠nh']\n",
        "\n",
        "    # S·ªë l∆∞·ª£ng c·∫ßn th√™m cho nh√£n 'Trung t√≠nh'\n",
        "    diff = target_count - len(neutral_data)\n",
        "\n",
        "    augmented_data = neutral_data.copy()\n",
        "\n",
        "    for _ in range(diff):\n",
        "        # Ch·ªçn ng·∫´u nhi√™n m·ªôt c√¢u v√† tƒÉng c∆∞·ªùng n√≥\n",
        "        row = neutral_data.sample(1)\n",
        "        augmented_text = augment_data(row['comment'].values[0], augmentation_type='both')\n",
        "        augmented_data = pd.concat([augmented_data,\n",
        "                                    pd.DataFrame({ 'comment': [augmented_text],\n",
        "                                                  'sentiment': ['Trung t√≠nh']})], ignore_index=True)\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n"
      ],
      "id": "initial_id",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-08T07:37:58.418255Z",
          "start_time": "2024-12-08T07:37:57.867885Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e95c9b8d2d118b0",
        "outputId": "baee43f9-1487-40c9-dae9-fd2a5118709a"
      },
      "cell_type": "code",
      "source": [
        "# S·ªë l∆∞·ª£ng m·∫´u m·ª•c ti√™u cho nh√£n 'Trung t√≠nh'\n",
        "target_neutral_count = 1000  # V√≠ d·ª• b·∫°n mu·ªën c√≥ 1000 c√¢u cho nh√£n 'Trung t√≠nh'\n",
        "\n",
        "# C√¢n b·∫±ng d·ªØ li·ªáu cho nh√£n \"Trung t√≠nh\"\n",
        "balanced_neutral_data = balance_neutral_data(data, target_neutral_count)\n",
        "\n",
        "# Th√™m l·∫°i d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng v√†o d·ªØ li·ªáu g·ªëc\n",
        "balanced_data = pd.concat([data[data['sentiment'] != 'Trung t√≠nh'], balanced_neutral_data], ignore_index=True)\n",
        "\n",
        "# Hi·ªÉn th·ªã d·ªØ li·ªáu sau khi c√¢n b·∫±ng\n",
        "print(\"D·ªØ li·ªáu sau khi c√¢n b·∫±ng nh√£n Trung t√≠nh:\")\n",
        "print(balanced_data['sentiment'].value_counts())\n",
        "\n",
        "# L∆∞u d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng\n",
        "balanced_data.to_csv(\"/content/drive/MyDrive/NLP/data/balanced_neutral_sentiment_data.csv\", index=False)"
      ],
      "id": "e95c9b8d2d118b0",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D·ªØ li·ªáu sau khi c√¢n b·∫±ng nh√£n Trung t√≠nh:\n",
            "sentiment\n",
            "T·ªët           1610\n",
            "T·ªá            1269\n",
            "Trung t√≠nh    1000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}